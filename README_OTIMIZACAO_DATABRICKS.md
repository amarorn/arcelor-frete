# üöÄ Otimiza√ß√£o de Performance com Databricks e Spark

## **Problema Identificado**

A vers√£o atual do script `analise_oportunidades_reducao.py` est√° **muito lenta** devido a:

1. **Processamento sequencial** com pandas
2. **Loops Python** para an√°lise de dados
3. **Opera√ß√µes n√£o vetorizadas**
4. **Uso ineficiente de mem√≥ria**
5. **Falta de paraleliza√ß√£o**

## **Solu√ß√£o: Migra√ß√£o para Databricks/Spark**

### **Performance Esperada: 10-100x mais r√°pido**

## **üìÅ Arquivos Criados**

### **1. `analise_oportunidades_reducao_spark.py`**
- Vers√£o Python standalone com Spark
- Para execu√ß√£o local ou em clusters

### **2. `notebooks/analise_oportunidades_databricks.py`**
- Notebook Databricks otimizado
- **RECOMENDADO** para produ√ß√£o

## **üîß Principais Otimiza√ß√µes Implementadas**

### **1. Configura√ß√µes Spark Avan√ßadas**
```python
# Adaptive Query Execution
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")

# Cache e Broadcast
spark.conf.set("spark.sql.adaptive.autoBroadcastJoinThreshold", "100485760")
```

### **2. Opera√ß√µes Vetorizadas**
- **Agrega√ß√µes Spark** em vez de loops Python
- **Window Functions** para sele√ß√£o de rotas representativas
- **Joins otimizados** com particionamento inteligente

### **3. Cache Estrat√©gico**
```python
# Cache de DataFrames intermedi√°rios
df_processed.cache()
df_analysis.cache()

# Limpeza autom√°tica de cache
df_processed.unpersist()
```

### **4. UDFs Otimizadas**
- **UDFs nativas Spark** para transforma√ß√µes
- **Broadcast de mapeamentos** para micro-regi√µes
- **Opera√ß√µes vetorizadas** em vez de apply()

## **üöÄ Como Executar no Databricks**

### **Op√ß√£o 1: Notebook (Recomendado)**
1. Abrir o notebook `analise_oportunidades_databricks.py`
2. Executar c√©lula por c√©lula
3. Acompanhar m√©tricas de performance

### **Op√ß√£o 2: Job Automatizado**
1. Criar Job no Databricks
2. Configurar cluster com recursos adequados
3. Executar notebook como tarefa

### **Op√ß√£o 3: Pipeline CI/CD**
1. Integrar com Azure DevOps/GitHub Actions
2. Deploy autom√°tico via Databricks Asset Bundles
3. Execu√ß√£o programada

## **‚öôÔ∏è Configura√ß√£o do Cluster**

### **Configura√ß√µes Recomendadas**
```json
{
  "cluster_name": "analise-oportunidades-otimizado",
  "spark_version": "13.3.x-scala2.12",
  "node_type_id": "Standard_DS3_v2",
  "num_workers": 2,
  "driver_node_type_id": "Standard_DS3_v2",
  "spark_conf": {
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.adaptive.coalescePartitions.enabled": "true",
    "spark.sql.adaptive.skewJoin.enabled": "true",
    "spark.sql.adaptive.localShuffleReader.enabled": "true",
    "spark.sql.adaptive.optimizeSkewedJoin.enabled": "true"
  }
}
```

### **Recursos M√≠nimos**
- **Driver**: 2 vCPUs, 7 GB RAM
- **Workers**: 2 vCPUs, 7 GB RAM cada
- **Storage**: 20 GB por n√≥

## **üìä Monitoramento de Performance**

### **M√©tricas Importantes**
1. **Tempo de execu√ß√£o** total
2. **N√∫mero de parti√ß√µes** ativas
3. **Status de cache** dos DataFrames
4. **Uso de mem√≥ria** e CPU
5. **Skew de dados** em joins

### **Comandos de Monitoramento**
```python
# Estat√≠sticas de parti√ß√µes
print(f"Parti√ß√µes: {df.rdd.getNumPartitions()}")

# Status de cache
print(f"Cache ativo: {df.is_cached}")

# Tamanho estimado
print(f"Tamanho: {df.count() * len(df.columns) * 8 / 1024 / 1024:.2f} MB")
```

## **üîç An√°lise de Performance**

### **Compara√ß√£o Esperada**
| M√©trica | Pandas | Spark | Melhoria |
|---------|--------|-------|----------|
| **Tempo total** | 300-600s | 30-60s | **10x** |
| **Processamento** | Sequencial | Paralelo | **50x** |
| **Mem√≥ria** | Alta | Otimizada | **5x** |
| **Escalabilidade** | Limitada | Ilimitada | **‚àû** |

### **Fatores de Melhoria**
1. **Paraleliza√ß√£o**: Processamento distribu√≠do
2. **Cache inteligente**: Reutiliza√ß√£o de dados
3. **Agrega√ß√µes otimizadas**: Spark SQL nativo
4. **Particionamento**: Balanceamento de carga
5. **Adaptive Query**: Otimiza√ß√£o autom√°tica

## **üìà Estrat√©gias de Otimiza√ß√£o Adicionais**

### **1. Particionamento de Dados**
```python
# Particionar por micro-regi√£o para joins mais eficientes
df_processed = df_processed.repartition(100, "microregiao_origem")
```

### **2. Broadcast de Lookups**
```python
# Broadcast de mapeamentos pequenos
microregion_broadcast = spark.sparkContext.broadcast(microregion_mapping)
```

### **3. Persist√™ncia Estrat√©gica**
```python
# Persistir DataFrames frequentemente usados
df_analysis.persist(StorageLevel.MEMORY_AND_DISK)
```

### **4. Otimiza√ß√£o de Joins**
```python
# Usar broadcast joins para tabelas pequenas
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "100485760")
```

## **üõ†Ô∏è Troubleshooting**

### **Problemas Comuns**

#### **1. Out of Memory**
```python
# Reduzir n√∫mero de parti√ß√µes
df = df.coalesce(50)

# Aumentar mem√≥ria do executor
spark.conf.set("spark.executor.memory", "8g")
```

#### **2. Skew de Dados**
```python
# Habilitar otimiza√ß√£o de skew
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes", "256MB")
```

#### **3. Performance de Joins**
```python
# Usar broadcast joins para tabelas pequenas
spark.conf.set("spark.sql.adaptive.autoBroadcastJoinThreshold", "100485760")
```

## **üìã Checklist de Implementa√ß√£o**

### **Fase 1: Prepara√ß√£o**
- [ ] Configurar cluster Databricks
- [ ] Instalar depend√™ncias (spark-excel)
- [ ] Configurar otimiza√ß√µes Spark
- [ ] Preparar dados de teste

### **Fase 2: Implementa√ß√£o**
- [ ] Migrar c√≥digo para Spark
- [ ] Implementar UDFs otimizadas
- [ ] Configurar cache estrat√©gico
- [ ] Otimizar joins e agrega√ß√µes

### **Fase 3: Testes**
- [ ] Testes de performance
- [ ] Valida√ß√£o de resultados
- [ ] Ajustes de configura√ß√£o
- [ ] Documenta√ß√£o final

### **Fase 4: Produ√ß√£o**
- [ ] Deploy em ambiente produtivo
- [ ] Monitoramento cont√≠nuo
- [ ] Otimiza√ß√µes iterativas
- [ ] Treinamento da equipe

## **üéØ Pr√≥ximos Passos**

### **Imediatos (1-2 semanas)**
1. **Testar** notebook no Databricks
2. **Comparar** performance com vers√£o atual
3. **Ajustar** configura√ß√µes conforme necess√°rio

### **M√©dio Prazo (1 m√™s)**
1. **Implementar** em produ√ß√£o
2. **Automatizar** execu√ß√£o via Jobs
3. **Monitorar** performance continuamente

### **Longo Prazo (3 meses)**
1. **Otimizar** ainda mais baseado em uso real
2. **Implementar** cache persistente
3. **Considerar** Delta Lake para dados hist√≥ricos

## **üí° Dicas de Performance**

1. **Sempre use cache()** para DataFrames reutilizados
2. **Monitore o n√∫mero de parti√ß√µes** - muito alto ou baixo pode prejudicar performance
3. **Use broadcast joins** para tabelas pequenas (< 100MB)
4. **Configure Adaptive Query Execution** para otimiza√ß√µes autom√°ticas
5. **Limpe cache** quando n√£o for mais necess√°rio

## **üìû Suporte**

Para d√∫vidas sobre otimiza√ß√£o:
- **Documenta√ß√£o Spark**: https://spark.apache.org/docs/latest/
- **Databricks Docs**: https://docs.databricks.com/
- **Comunidade**: Stack Overflow com tags [apache-spark], [databricks]

---

**üéâ Com essas otimiza√ß√µes, sua an√°lise ser√° 10-100x mais r√°pida!**
